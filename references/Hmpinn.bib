
@misc{kiyani_which_2025,
	title = {Which {Optimizer} {Works} {Best} for {Physics}-{Informed} {Neural} {Networks} and {Kolmogorov}-{Arnold} {Networks}?},
	url = {http://arxiv.org/abs/2501.16371},
	doi = {10.48550/arXiv.2501.16371},
	abstract = {Physics-Informed Neural Networks (PINNs) have revolutionized the computation of PDE solutions by integrating partial differential equations (PDEs) into the neural network's training process as soft constraints, becoming an important component of the scientific machine learning (SciML) ecosystem. More recently, physics-informed Kolmogorv-Arnold networks (PIKANs) have also shown to be effective and comparable in accuracy with PINNs. In their current implementation, both PINNs and PIKANs are mainly optimized using first-order methods like Adam, as well as quasi-Newton methods such as BFGS and its low-memory variant, L-BFGS. However, these optimizers often struggle with highly non-linear and non-convex loss landscapes, leading to challenges such as slow convergence, local minima entrapment, and (non)degenerate saddle points. In this study, we investigate the performance of Self-Scaled BFGS (SSBFGS), Self-Scaled Broyden (SSBroyden) methods and other advanced quasi-Newton schemes, including BFGS and L-BFGS with different line search strategies approaches. These methods dynamically rescale updates based on historical gradient information, thus enhancing training efficiency and accuracy. We systematically compare these optimizers -- using both PINNs and PIKANs -- on key challenging linear, stiff, multi-scale and non-linear PDEs, including the Burgers, Allen-Cahn, Kuramoto-Sivashinsky, and Ginzburg-Landau equations. Our findings provide state-of-the-art results with orders-of-magnitude accuracy improvements without the use of adaptive weights or any other enhancements typically employed in PINNs. More broadly, our results reveal insights into the effectiveness of second-order optimization strategies in significantly improving the convergence and accurate generalization of PINNs and PIKANs.},
	urldate = {2025-04-30},
	publisher = {arXiv},
	author = {Kiyani, Elham and Shukla, Khemraj and Urbán, Jorge F. and Darbon, Jérôme and Karniadakis, George Em},
	month = apr,
	year = {2025},
	note = {arXiv:2501.16371 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Preprint PDF:C\:\\Users\\mattb\\Zotero\\storage\\YS4R4WLI\\Kiyani et al. - 2025 - Which Optimizer Works Best for Physics-Informed Neural Networks and Kolmogorov-Arnold Networks.pdf:application/pdf;Snapshot:C\:\\Users\\mattb\\Zotero\\storage\\59Y8PUZ4\\2501.html:text/html},
}

@article{lakkis_finite_2011,
	title = {A finite element method for second order nonvariational elliptic problems},
	volume = {33},
	issn = {1064-8275, 1095-7197},
	url = {http://arxiv.org/abs/1003.0292},
	doi = {10.1137/100787672},
	abstract = {We propose a numerical method to approximate the solution of second order elliptic problems in nonvariational form. The method is of Galerkin type using conforming finite elements and applied directly to the nonvariational (nondivergence) form of a second order linear elliptic problem. The key tools are an appropriate concept of a 'finite element Hessian' and a Schur complement approach to solving the resulting linear algebra problem. The method is illustrated with computational experiments on three linear and one quasilinear PDE, all in nonvariational form.},
	number = {2},
	urldate = {2025-05-02},
	journal = {SIAM Journal on Scientific Computing},
	author = {Lakkis, Omar and Pryer, Tristan},
	month = jan,
	year = {2011},
	note = {arXiv:1003.0292 [math]},
	keywords = {Mathematics - Numerical Analysis},
	pages = {786--801},
	file = {Full Text PDF:C\:\\Users\\mattb\\Zotero\\storage\\HM584ZLE\\Lakkis and Pryer - 2011 - A finite element method for second order nonvariational elliptic problems.pdf:application/pdf;Snapshot:C\:\\Users\\mattb\\Zotero\\storage\\S27PKACQ\\1003.html:text/html},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	urldate = {2025-05-10},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
	file = {Full Text PDF:C\:\\Users\\mattb\\Zotero\\storage\\EW2AEBSA\\Raissi et al. - 2019 - Physics-informed neural networks A deep learning framework for solving forward and inverse problems.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\mattb\\Zotero\\storage\\7ZBK8FVA\\S0021999118307125.html:text/html},
}

@article{deguchi_dynamic_2023,
	title = {Dynamic \& norm-based weights to normalize imbalance in back-propagated gradients of physics-informed neural networks},
	volume = {7},
	issn = {2399-6528},
	url = {https://dx.doi.org/10.1088/2399-6528/ace416},
	doi = {10.1088/2399-6528/ace416},
	abstract = {Physics-Informed Neural Networks (PINNs) have been a promising machine learning model for evaluating various physical problems. Despite their success in solving many types of partial differential equations (PDEs), some problems have been found to be difficult to learn, implying that the baseline PINNs is biased towards learning the governing PDEs while relatively neglecting given initial or boundary conditions. In this work, we propose Dynamically Normalized Physics-Informed Neural Networks (DN-PINNs), a method to train PINNs while evenly distributing multiple back-propagated gradient components. DN-PINNs determine the relative weights assigned to initial or boundary condition losses based on gradient norms, and the weights are updated dynamically during training. Through several numerical experiments, we demonstrate that DN-PINNs effectively avoids the imbalance in multiple gradients and improves the inference accuracy while keeping the additional computational cost within a reasonable range. Furthermore, we compare DN-PINNs with other PINNs variants and empirically show that DN-PINNs is competitive with or outperforms them. In addition, since DN-PINN uses exponential decay to update the relative weight, the weights obtained are biased toward the initial values. We study this initialization bias and show that a simple bias correction technique can alleviate this problem.},
	language = {en},
	number = {7},
	urldate = {2025-05-10},
	journal = {Journal of Physics Communications},
	author = {Deguchi, Shota and Asai, Mitsuteru},
	month = jul,
	year = {2023},
	note = {Publisher: IOP Publishing},
	pages = {075005},
	file = {IOP Full Text PDF:C\:\\Users\\mattb\\Zotero\\storage\\G77YBIME\\Deguchi and Asai - 2023 - Dynamic & norm-based weights to normalize imbalance in back-propagated gradients of physics-informed.pdf:application/pdf},
}

@misc{kast_positional_2023,
	title = {Positional {Embeddings} for {Solving} {PDEs} with {Evolutional} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2308.03461},
	doi = {10.48550/arXiv.2308.03461},
	abstract = {This work extends the paradigm of evolutional deep neural networks (EDNNs) to solving parametric time-dependent partial differential equations (PDEs) on domains with geometric structure. By introducing positional embeddings based on eigenfunctions of the Laplace-Beltrami operator, geometric properties are encoded intrinsically and Dirichlet, Neumann and periodic boundary conditions of the PDE solution are enforced directly through the neural network architecture. The proposed embeddings lead to improved error convergence for static PDEs and extend EDNNs towards computational domains of realistic complexity. Several steps are taken to improve performance of EDNNs: Solving the EDNN update equation with a Krylov solver avoids the explicit assembly of Jacobians and enables scaling to larger neural networks. Computational efficiency is further improved by an ad-hoc active sampling scheme that uses the PDE dynamics to effectively sample collocation points. A modified linearly implicit Rosenbrock method is proposed to alleviate the time step requirements of stiff PDEs. Lastly, a completely training-free approach, which automatically enforces initial conditions and only requires time integration, is compared against EDNNs that are trained on the initial conditions. We report results for the Korteweg-de Vries equation, a nonlinear heat equation and (nonlinear) advection-diffusion problems on domains with and without holes and various boundary conditions, to demonstrate the effectiveness of the method. The numerical results highlight EDNNs as a promising surrogate model for parametrized PDEs with slow decaying Kolmogorov n-width.},
	urldate = {2025-05-10},
	publisher = {arXiv},
	author = {Kast, Mariella and Hesthaven, Jan S.},
	month = aug,
	year = {2023},
	note = {arXiv:2308.03461 [math]},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Numerical Analysis},
	file = {Preprint PDF:C\:\\Users\\mattb\\Zotero\\storage\\NJDRYEXS\\Kast and Hesthaven - 2023 - Positional Embeddings for Solving PDEs with Evolutional Deep Neural Networks.pdf:application/pdf;Snapshot:C\:\\Users\\mattb\\Zotero\\storage\\RRD2H2RD\\2308.html:text/html},
}

@article{noauthor_pdf_nodate,
	title = {({PDF}) {Understanding} the difficulty of training deep feedforward neural networks},
	url = {https://www.researchgate.net/publication/215616968_Understanding_the_difficulty_of_training_deep_feedforward_neural_networks},
	abstract = {PDF {\textbar} Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-05-10},
	journal = {ResearchGate},
	file = {PDF:C\:\\Users\\mattb\\Zotero\\storage\\3XVF39IZ\\(PDF) Understanding the difficulty of training deep feedforward neural networks.pdf:application/pdf;Snapshot:C\:\\Users\\mattb\\Zotero\\storage\\6HLKFWHK\\215616968_Understanding_the_difficulty_of_training_deep_feedforward_neural_networks.html:text/html},
}

@book{gilbarg_elliptic_2001,
	address = {Berlin, Heidelberg},
	series = {Classics in {Mathematics}},
	title = {Elliptic {Partial} {Differential} {Equations} of {Second} {Order}},
	volume = {224},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-540-41160-4 978-3-642-61798-0},
	url = {http://link.springer.com/10.1007/978-3-642-61798-0},
	language = {en},
	urldate = {2025-05-10},
	publisher = {Springer},
	author = {Gilbarg, David and Trudinger, Neil S.},
	year = {2001},
	doi = {10.1007/978-3-642-61798-0},
	keywords = {2000, 25Gxx, 35Jxx, Classification, differential equation, Elliptic PDE, Interpolation, Mathematical, Mathematical Subject Classification 2000, maximum principle, nonlinear analysis, partial differential equation, partial differential equations, Sobolev space, Subject, YellowSale2006},
	file = {Full Text PDF:C\:\\Users\\mattb\\Zotero\\storage\\9J8WHWW6\\Gilbarg and Trudinger - 2001 - Elliptic Partial Differential Equations of Second Order.pdf:application/pdf},
}

@misc{wang_experts_2023,
	title = {An {Expert}'s {Guide} to {Training} {Physics}-informed {Neural} {Networks}},
	url = {http://arxiv.org/abs/2308.08468},
	doi = {10.48550/arXiv.2308.08468},
	abstract = {Physics-informed neural networks (PINNs) have been popularized as a deep learning framework that can seamlessly synthesize observational data and partial differential equation (PDE) constraints. Their practical effectiveness however can be hampered by training pathologies, but also oftentimes by poor choices made by users who lack deep learning expertise. In this paper we present a series of best practices that can significantly improve the training efficiency and overall accuracy of PINNs. We also put forth a series of challenging benchmark problems that highlight some of the most prominent difficulties in training PINNs, and present comprehensive and fully reproducible ablation studies that demonstrate how different architecture choices and training strategies affect the test accuracy of the resulting models. We show that the methods and guiding principles put forth in this study lead to state-of-the-art results and provide strong baselines that future studies should use for comparison purposes. To this end, we also release a highly optimized library in JAX that can be used to reproduce all results reported in this paper, enable future research studies, as well as facilitate easy adaptation to new use-case scenarios.},
	urldate = {2025-05-10},
	publisher = {arXiv},
	author = {Wang, Sifan and Sankaran, Shyam and Wang, Hanwen and Perdikaris, Paris},
	month = aug,
	year = {2023},
	note = {arXiv:2308.08468 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Numerical Analysis, Physics - Computational Physics},
	file = {Preprint PDF:C\:\\Users\\mattb\\Zotero\\storage\\Y37HJZBB\\Wang et al. - 2023 - An Expert's Guide to Training Physics-informed Neural Networks.pdf:application/pdf;Snapshot:C\:\\Users\\mattb\\Zotero\\storage\\N66VYA3X\\2308.html:text/html},
}

@article{hinz_use_2024,
	title = {On the use of elliptic {PDEs} for the parameterisation of planar multipatch domains},
	volume = {40},
	issn = {1435-5663},
	url = {https://doi.org/10.1007/s00366-024-01997-x},
	doi = {10.1007/s00366-024-01997-x},
	abstract = {This paper presents a parameterisation framework based on (inverted) elliptic PDEs for addressing the planar parameterisation problem of finding a valid description of the domain’s interior given no more than a spline-based description of its boundary contours. The framework is geared towards isogeometric analysis (IGA) applications wherein the physical domain is comprised of more than four sides, hence requiring more than one patch. We adopt the concept of harmonic maps and propose several PDE-based problem formulations capable of finding a valid map between a convex parametric multipatch domain and the piecewise-smooth physical domain with an equal number of sides. In line with the isoparametric paradigm of IGA, we treat the parameterisation problem using techniques that are characteristic for the analysis step. As such, this study proposes several IGA-based numerical algorithms for the problem’s governing equations that can be effortlessly integrated into a well-developed IGA software suite. We augment the framework with mechanisms that enable controlling the parametric properties of the outcome. Parametric control is accomplished by, among other techniques, the introduction of a curvilinear coordinate system in the convex parametric domain, for which more general elliptic PDEs are adopted. Depending on the application, parametric control allows for building desired features into the computed map, such as homogeneous cell sizes or boundary layers.},
	language = {en},
	number = {6},
	urldate = {2025-05-16},
	journal = {Engineering with Computers},
	author = {Hinz, Jochen and Buffa, Annalisa},
	month = dec,
	year = {2024},
	keywords = {Applied Dynamical Systems, Computer Modelling, Engineering Mathematics, Harmonic maps, Isogeometric analysis, Parameterisation techniques, Partial Differential Equations, Partial Differential Equations on Manifolds, Theory and Algorithms for Application Domains},
	pages = {3735--3764},
	file = {Full Text PDF:C\:\\Users\\mattb\\Zotero\\storage\\9L4S8MSA\\Hinz and Buffa - 2024 - On the use of elliptic PDEs for the parameterisation of planar multipatch domains.pdf:application/pdf},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2025-05-26},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\mattb\\Zotero\\storage\\AIYUI5W4\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;Snapshot:C\:\\Users\\mattb\\Zotero\\storage\\BYDPFUNA\\1412.html:text/html},
}

@article{liu_limited_1989,
	title = {On the limited memory {BFGS} method for large scale optimization},
	volume = {45},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/BF01589116},
	doi = {10.1007/BF01589116},
	abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
	language = {en},
	number = {1},
	urldate = {2025-05-26},
	journal = {Mathematical Programming},
	author = {Liu, Dong C. and Nocedal, Jorge},
	month = aug,
	year = {1989},
	keywords = {Calculus of Variations and Optimization, conjugate gradient method, Continuous Optimization, Discrete Optimization, Large scale nonlinear optimization, limited memory methods, Linear Algebra, Numerical Analysis, Optimization, partitioned quasi-Newton method},
	pages = {503--528},
	file = {Full Text PDF:C\:\\Users\\mattb\\Zotero\\storage\\EQSLZUYU\\Liu and Nocedal - 1989 - On the limited memory BFGS method for large scale optimization.pdf:application/pdf},
}

@misc{hendrycks_gaussian_2023,
	title = {Gaussian {Error} {Linear} {Units} ({GELUs})},
	url = {http://arxiv.org/abs/1606.08415},
	doi = {10.48550/arXiv.1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{\textgreater}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = jun,
	year = {2023},
	note = {arXiv:1606.08415 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\mattb\\Zotero\\storage\\FVF2IZD7\\Hendrycks and Gimpel - 2023 - Gaussian Error Linear Units (GELUs).pdf:application/pdf;Snapshot:C\:\\Users\\mattb\\Zotero\\storage\\J37GRLTZ\\1606.html:text/html},
}
